{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, tqdm\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "from sacrebleu.metrics import BLEU\n",
    "from speech_to_speech_translation.text_cleaner.cleaners import english_cleaners\n",
    "\n",
    "src_lang, tgt_lang = \"fr\", \"en\"\n",
    "gen_subset = \"valid\"\n",
    "# transcript_txt_path = transcript_txt_path = \"/opt/data/private/dsy/project/model/multimodal_S2UT/mm_s2ut/checkpoints/enhanced_fr-en/inference/tts_transcript.txt\"\n",
    "transcript_txt_path = \"/opt/data/private/dsy/project/model/multimodal_S2UT/mm_s2ut/checkpoints/textless_fr-en/inference/tts_transcript.txt\"\n",
    "wav_dir = f\"/opt/data/private/dsy/project/dataset/multi30k-dataset/data/speech/16khz_wav/{tgt_lang}/{gen_subset}\"\n",
    "ref_txt = f\"/opt/data/private/dsy/project/dataset/multi30k-dataset/data/text-clean/{gen_subset}.{tgt_lang}\"\n",
    "tsv_path = f\"/opt/data/private/dsy/project/dataset/multi30k-dataset/data/speech/format_data/fr-en/{gen_subset}.tsv\"\n",
    "\n",
    "tsv = pd.read_csv(tsv_path, sep='\\t')\n",
    "ref_id_list = tsv[\"id\"].tolist()\n",
    "ref_list = []\n",
    "with open(ref_txt, mode=\"r+\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        ref_list.append(line)\n",
    "hyp_list = []\n",
    "hyp_ref_list = []\n",
    "with open(transcript_txt_path, mode=\"r+\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        hyp_list.append(line)\n",
    "        hyp_ref_list.append([line, ref_list[ref_id_list[len(hyp_list) - 1] - 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 24.95 58.7/35.2/21.7/13.6 (BP = 0.893 ratio = 0.898 hyp_len = 11938 ref_len = 13289)\n"
     ]
    }
   ],
   "source": [
    "def remove_end_punc(line):\n",
    "    if line.endswith(\" .\"):\n",
    "        line = line[: len(line) - 2]\n",
    "    return line\n",
    "\n",
    "# print(len(ref_list), len(hyp_list))\n",
    "assert len(ref_list) == len(hyp_list)\n",
    "for i in range(len(ref_list)):\n",
    "    hyp, ref = hyp_ref_list[i]\n",
    "    hyp = english_cleaners(hyp)\n",
    "    ref = english_cleaners(ref)\n",
    "    # hyp, ref = remove_end_punc(hyp), remove_end_punc(ref)\n",
    "    hyp_ref_list[i] = [hyp, ref]\n",
    "    # print(ref_id_list[i])\n",
    "    # print(\"hyp: \", hyp)\n",
    "    # print(\"ref: \", ref)\n",
    "    # print()\n",
    "bleu_score = sacrebleu.corpus_bleu(\n",
    "    [hyp for hyp, _ in hyp_ref_list], \n",
    "    # [ref for _, ref in hyp_ref_list], \n",
    "    [[ref for _, ref in hyp_ref_list]]\n",
    ")\n",
    "print(bleu_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原始音频转录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch, torchaudio, tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/root/autodl-tmp/liuwenrui/project/model/transformer_ckpt/wav2vec2-large-960h-lv60-self\"\n",
    "\n",
    "def generate_transcription(\n",
    "    tts_wav_dir: str, \n",
    "    transcript_txt: str, \n",
    "):\n",
    "    transcriptions = []\n",
    "    # load model and processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    tts_wav_files = [file for file in os.listdir(tts_wav_dir) if file.endswith(\".wav\")]\n",
    "    # tts_wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "    tts_wav_files.sort(key=lambda x: int(x.split('.')[0]))\n",
    "    for wav_file in tqdm.tqdm(tts_wav_files):\n",
    "        # if not wav_file.endswith(\".wav\"):\n",
    "        #     continue\n",
    "        wav_path = os.path.join(tts_wav_dir, wav_file)\n",
    "        audio, sr = torchaudio.load(wav_path)\n",
    "        audio = audio[0]\n",
    "        # tokenize\n",
    "        input_values = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "        input_values = input_values.to(device)\n",
    "        # retrieve logits\n",
    "        logits = model(input_values).logits\n",
    "        logits = logits.cpu()\n",
    "        # take argmax and decode\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        transcription = transcription[0]\n",
    "        transcriptions.append(transcription)\n",
    "    with open(transcript_txt, mode=\"w+\") as f:\n",
    "        f.write('\\n'.join(transcriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at /root/autodl-tmp/liuwenrui/project/model/transformer_ckpt/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 29000/29000 [11:43<00:00, 41.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# subset = \"valid.en\"\n",
    "# subset = \"test.2016.en\"\n",
    "# subset = \"test.2017.en\"\n",
    "# subset = \"test.coco.en\"\n",
    "subset = \"train.en\"\n",
    "audio_dir = f\"/root/autodl-tmp/liuwenrui/project/dataset/multi30k-dataset/data/speech/16khz_wav/{subset}\"\n",
    "output_dir = \"/root/autodl-tmp/liuwenrui/project/model/multimodal_S2UT/mm_s2ut/checkpoints/wav2vec2-en/ori\"\n",
    "generate_transcription(\n",
    "    tts_wav_dir=audio_dir, \n",
    "    transcript_txt=os.path.join(output_dir, f\"{subset}.txt\"), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 89.44 95.0/91.3/87.8/84.1 (BP = 1.000 ratio = 1.008 hyp_len = 4899 ref_len = 4860)\n",
      "word_error_rate = 0.05576131687242798\n"
     ]
    }
   ],
   "source": [
    "import os, json, tqdm\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "import evaluate\n",
    "from sacrebleu.metrics import BLEU\n",
    "from speech_to_speech_translation.text_cleaner.cleaners import english_cleaners\n",
    "wer_evaluator = evaluate.load(\"wer\")\n",
    "\n",
    "# subset = \"train.en\"\n",
    "# subset = \"valid.en\"\n",
    "# subset = \"test.2016.en\"\n",
    "# subset = \"test.2017.en\"\n",
    "subset = \"test.coco.en\"\n",
    "\n",
    "transcript_txt_path = os.path.join(output_dir, f\"{subset}.txt\")\n",
    "ref_txt = f\"/root/autodl-tmp/liuwenrui/project/dataset/multi30k-dataset/data/text-clean/{subset}\"\n",
    "\n",
    "ref_list = []\n",
    "with open(ref_txt, mode=\"r+\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        ref_list.append(line)\n",
    "hyp_list = []\n",
    "with open(transcript_txt_path, mode=\"r+\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        hyp_list.append(line)\n",
    "\n",
    "def remove_end_punc(line):\n",
    "    if line.endswith(\" .\"):\n",
    "        line = line[: len(line) - 2]\n",
    "    return line\n",
    "\n",
    "# print(len(ref_list), len(hyp_list))\n",
    "assert len(ref_list) == len(hyp_list)\n",
    "for i in range(len(ref_list)):\n",
    "    hyp, ref = hyp_list[i], ref_list[i]\n",
    "    hyp = english_cleaners(hyp)\n",
    "    ref = english_cleaners(ref)\n",
    "    hyp, ref = remove_end_punc(hyp), remove_end_punc(ref)\n",
    "    hyp_list[i], ref_list[i] = hyp, ref\n",
    "    # print(\"hyp: \", hyp)\n",
    "    # print(\"ref: \", ref)\n",
    "    # print()\n",
    "bleu_score = sacrebleu.corpus_bleu(\n",
    "    [hyp for hyp in hyp_list], \n",
    "    [[ref for ref in ref_list]]\n",
    ")\n",
    "print(bleu_score)\n",
    "wer = wer_evaluator.compute(predictions=hyp_list, references=ref_list)\n",
    "print(f\"word_error_rate = {wer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original audio transcript\n",
    "| subset       | WER   | ASR BLEU |\n",
    "| -----------  | ----- | ----------- |\n",
    "| train.en     | 6.22% | BLEU = 88.44 95.1/90.6/86.4/82.2 (BP = 1.000 ratio = 1.000 hyp_len = 349629 ref_len = 349505) |\n",
    "| valid.en     | 6.21% | BLEU = 88.49 95.1/90.7/86.5/82.2 (BP = 1.000 ratio = 1.001 hyp_len = 12331 ref_len = 12323) |\n",
    "| test.2016.en | 5.49% | BLEU = 89.70 95.6/91.5/87.9/84.2 (BP = 1.000 ratio = 1.001 hyp_len = 12028 ref_len = 12012) |\n",
    "| test.2017.en | 5.34% | BLEU = 90.03 95.7/91.8/88.3/84.8 (BP = 1.000 ratio = 1.001 hyp_len = 10632 ref_len = 10617) |\n",
    "| test.coco.en | 5.57% | BLEU = 89.44 95.0/91.3/87.8/84.1 (BP = 1.000 ratio = 1.008 hyp_len = 4899 ref_len = 4860)   |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit HiFiGAN重建音频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 23:31:17 | INFO | fairseq.file_utils | loading archive file /root/autodl-tmp/liuwenrui/project/model/transformer_ckpt/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\n",
      "2023-05-02 23:31:17 | INFO | fairseq.models.text_to_speech.vocoder | loaded CodeHiFiGAN checkpoint from /root/autodl-tmp/liuwenrui/project/model/transformer_ckpt/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur/model.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing weight norm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [01:49<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import os, json, tqdm\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "from fairseq import hub_utils\n",
    "from fairseq.models.text_to_speech import CodeHiFiGANVocoder\n",
    "from fairseq.models.text_to_speech.hub_interface import VocoderHubInterface\n",
    "\n",
    "# subset = \"train.en\"\n",
    "# subset = \"valid.en\"\n",
    "# subset = \"test.2016.en\"\n",
    "# subset = \"test.2017.en\"\n",
    "subset = \"test.coco.en\"\n",
    "tsv_path = f\"/root/autodl-tmp/liuwenrui/project/dataset/multi30k-dataset/data/speech/format_data/fr-en_enhanced/{'.'.join(subset.split('.')[:-1])}.tsv\"\n",
    "output_dir = f\"/root/autodl-tmp/liuwenrui/project/model/multimodal_S2UT/mm_s2ut/checkpoints/unit_hifigan/{subset}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cache_dir = \"/root/autodl-tmp/liuwenrui/project/model/transformer_ckpt/unit_hifigan_mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj_dur\"\n",
    "x = hub_utils.from_pretrained(\n",
    "    cache_dir,\n",
    "    \"model.pt\",\n",
    "    \".\",\n",
    "    archive_map=CodeHiFiGANVocoder.hub_models(),\n",
    "    config_yaml=\"config.json\",\n",
    "    fp16=False,\n",
    "    is_vocoder=True,\n",
    ")\n",
    "with open(f\"{x['args']['data']}/config.json\") as f:\n",
    "    vocoder_cfg = json.load(f)\n",
    "assert (\n",
    "    len(x[\"args\"][\"model_path\"]) == 1\n",
    "), \"Too many vocoder models in the input\"\n",
    "vocoder = CodeHiFiGANVocoder(x[\"args\"][\"model_path\"][0], vocoder_cfg)\n",
    "tts_model = VocoderHubInterface(vocoder_cfg, vocoder)\n",
    "\n",
    "tsv = pd.read_csv(tsv_path, sep='\\t')\n",
    "for i, row in tqdm.tqdm(tsv.iterrows(), total=tsv.shape[0]):\n",
    "    unit = row[\"tgt_text\"]\n",
    "    tts_sample = tts_model.get_model_input(unit)\n",
    "    wav, sr = tts_model.get_prediction(tts_sample)\n",
    "    sf.write(os.path.join(output_dir, f\"{row['id']}.wav\"), wav, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at /root/autodl-tmp/liuwenrui/project/model/transformer_ckpt/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 461/461 [00:10<00:00, 44.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch, torchaudio, tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/root/autodl-tmp/liuwenrui/project/model/transformer_ckpt/wav2vec2-large-960h-lv60-self\"\n",
    "\n",
    "def generate_transcription(\n",
    "    tts_wav_dir: str, \n",
    "    transcript_txt: str, \n",
    "):\n",
    "    transcriptions = []\n",
    "    # load model and processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    tts_wav_files = [file for file in os.listdir(tts_wav_dir) if file.endswith(\".wav\")]\n",
    "    # tts_wav_files.sort(key=lambda x: int(x.split('_')[0]))\n",
    "    tts_wav_files.sort(key=lambda x: int(x.split('.')[0]))\n",
    "    for wav_file in tqdm.tqdm(tts_wav_files):\n",
    "        # if not wav_file.endswith(\".wav\"):\n",
    "        #     continue\n",
    "        wav_path = os.path.join(tts_wav_dir, wav_file)\n",
    "        audio, sr = torchaudio.load(wav_path)\n",
    "        audio = audio[0]\n",
    "        # tokenize\n",
    "        input_values = processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "        input_values = input_values.to(device)\n",
    "        # retrieve logits\n",
    "        logits = model(input_values).logits\n",
    "        logits = logits.cpu()\n",
    "        # take argmax and decode\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)\n",
    "        transcription = transcription[0]\n",
    "        transcriptions.append(transcription)\n",
    "    with open(transcript_txt, mode=\"w+\") as f:\n",
    "        f.write('\\n'.join(transcriptions))\n",
    "\n",
    "audio_dir = output_dir\n",
    "output_dir = \"/root/autodl-tmp/liuwenrui/project/model/multimodal_S2UT/mm_s2ut/checkpoints/wav2vec2-en/reconstruct\"\n",
    "generate_transcription(\n",
    "    tts_wav_dir=audio_dir, \n",
    "    transcript_txt=os.path.join(output_dir, f\"{subset}.txt\"), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.coco.en\n",
      "BLEU = 72.03 86.1/76.6/68.3/60.5 (BP = 0.997 ratio = 0.997 hyp_len = 4844 ref_len = 4860)\n",
      "word_error_rate = 0.15864197530864196\n"
     ]
    }
   ],
   "source": [
    "import os, json, tqdm\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "import evaluate\n",
    "from sacrebleu.metrics import BLEU\n",
    "from speech_to_speech_translation.text_cleaner.cleaners import english_cleaners\n",
    "wer_evaluator = evaluate.load(\"wer\")\n",
    "\n",
    "# subset = \"train.en\"\n",
    "# subset = \"valid.en\"\n",
    "# subset = \"test.2016.en\"\n",
    "# subset = \"test.2017.en\"\n",
    "# subset = \"test.coco.en\"\n",
    "\n",
    "transcript_txt_path = os.path.join(output_dir, f\"{subset}.txt\")\n",
    "ref_txt = f\"/root/autodl-tmp/liuwenrui/project/dataset/multi30k-dataset/data/text-clean/{subset}\"\n",
    "\n",
    "ref_list = []\n",
    "with open(ref_txt, mode=\"r+\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        ref_list.append(line)\n",
    "hyp_list = []\n",
    "with open(transcript_txt_path, mode=\"r+\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        hyp_list.append(line)\n",
    "\n",
    "def remove_end_punc(line):\n",
    "    if line.endswith(\" .\"):\n",
    "        line = line[: len(line) - 2]\n",
    "    return line\n",
    "\n",
    "# print(len(ref_list), len(hyp_list))\n",
    "assert len(ref_list) == len(hyp_list)\n",
    "for i in range(len(ref_list)):\n",
    "    hyp, ref = hyp_list[i], ref_list[i]\n",
    "    hyp = english_cleaners(hyp)\n",
    "    ref = english_cleaners(ref)\n",
    "    hyp, ref = remove_end_punc(hyp), remove_end_punc(ref)\n",
    "    hyp_list[i], ref_list[i] = hyp, ref\n",
    "    # print(\"hyp: \", hyp)\n",
    "    # print(\"ref: \", ref)\n",
    "    # print()\n",
    "bleu_score = sacrebleu.corpus_bleu(\n",
    "    [hyp for hyp in hyp_list], \n",
    "    [[ref for ref in ref_list]]\n",
    ")\n",
    "print(subset)\n",
    "print(bleu_score)\n",
    "wer = wer_evaluator.compute(predictions=hyp_list, references=ref_list)\n",
    "print(f\"word_error_rate = {wer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstruct transcript\n",
    "| subset       | WER   | ASR BLEU |\n",
    "| -----------  | ----- | ----------- |\n",
    "| train.en     | 15.42% | BLEU = 71.79 87.0/77.0/68.4/60.6 (BP = 0.989 ratio = 0.989 hyp_len = 345592 ref_len = 349505) |\n",
    "| valid.en     | 14.90% | BLEU = 72.85 87.3/77.8/69.5/61.7 (BP = 0.992 ratio = 0.992 hyp_len = 12222 ref_len = 12323) |\n",
    "| test.2016.en | 15.29% | BLEU = 71.85 86.9/76.9/68.3/60.5 (BP = 0.991 ratio = 0.992 hyp_len = 11910 ref_len = 12012) |\n",
    "| test.2017.en | 16.07% | BLEU = 70.88 86.0/75.7/67.3/59.7 (BP = 0.991 ratio = 0.991 hyp_len = 10524 ref_len = 10617) |\n",
    "| test.coco.en | 15.86% | BLEU = 72.03 86.1/76.6/68.3/60.5 (BP = 0.997 ratio = 0.997 hyp_len = 4844 ref_len = 4860) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
